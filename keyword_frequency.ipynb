{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword Ordering\n",
        "This code analyses text documenst and imports excel files. It then divides then creates frequency counts based on previously developed keywords."
      ],
      "metadata": {
        "id": "1j1OVi1cmSC2"
      },
      "id": "1j1OVi1cmSC2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2de19263",
      "metadata": {
        "id": "2de19263"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from PyPDF2 import PdfReader\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_tokens(pdf_path):\n",
        "    # Read PDF file and preprocess text\n",
        "    # pdf_path = \"/Users/jz/Desktop/CMU/lyx_pdfReader/Nexteer_report.pdf\"\n",
        "    pdf_file = open(pdf_path, 'rb')\n",
        "    pdf_reader = PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "    pdf_file.close()\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "def collect(filepath):\n",
        "    pdf_files = glob.glob(os.path.join(filepath, \"*.pdf\"))\n",
        "    collections = {}\n",
        "    for file in pdf_files:\n",
        "        splits = file.split('/')\n",
        "        filename = splits[len(splits)-1].split('_')[0]\n",
        "        collections[filename] = get_tokens(file)\n",
        "    return collections\n",
        "\n",
        "\n",
        "\n",
        "# Read Excel file and create keyword sets based on categories\n",
        "category_df = pd.read_excel(\"/Users/jenniferlaura/Documents/CMU - MISM-12/Spring 2023/Capstone Project/keywords.xlsx\", sheet_name='Weights')    ## PATH TO THE WEIGHTS DOCUMENT\n",
        "keywords_df = pd.read_excel(\"/Users/jenniferlaura/Documents/CMU - MISM-12/Spring 2023/Capstone Project/keywords.xlsx\", sheet_name='Keywords')   ## PATH TO THE KEYWORDS DOCUMENT\n",
        "\n",
        "# Path of the folder containing pdfs\n",
        "filepath = \"/Users/jenniferlaura/Documents/CMU - MISM-12/Spring 2023/Capstone Project/pdfs\"\n",
        "\n",
        "\n",
        "# get all tokens for each pdf file in the filepath\n",
        "weight_dict = {}\n",
        "subcategory_dict = {}\n",
        "keywords_dict = {}\n",
        "\n",
        "for i in range(len(category_df.columns) // 2):\n",
        "    category_name = category_df.columns[i * 2]\n",
        "    sub_category = category_df[category_df.columns[i * 2]].dropna().tolist()\n",
        "    weight = category_df[category_df.columns[i * 2 + 1]].dropna().tolist()\n",
        "    map = {k:v for k,v in zip(sub_category, weight)}\n",
        "    weight_dict[category_name] = map\n",
        "    for sub in sub_category:\n",
        "        subcategory_dict[sub] = category_name\n",
        "\n",
        "for i in range(len(keywords_df.columns)):\n",
        "    category = keywords_df.columns[i]\n",
        "    keywords = keywords_df[category].dropna().tolist()\n",
        "    for keyword in keywords:\n",
        "        keywords_dict[keyword] = category\n",
        "\n",
        "collections = collect(filepath)\n",
        "columnname = [\"keyword\", \"subcategory\", \"category\"]\n",
        "for companyname in collections.keys():\n",
        "    columnname.extend([companyname+\"_freq\", companyname + \"_freq_weight\"])\n",
        "df1 = pd.DataFrame(columns = columnname)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for keyword, subcategory in keywords_dict.items():\n",
        "    category = subcategory_dict[subcategory]\n",
        "    dict = {\"keyword\":keyword, \"subcategory\":subcategory, \"category\":category}\n",
        "    for companyname, tokens in collections.items():\n",
        "        frequency = 0\n",
        "        words = word_tokenize(keyword.lower())\n",
        "        for i in range(len(tokens) - len(words) + 1):\n",
        "            if all(tokens[i + j] == lemmatizer.lemmatize(words[j]) for j in range(len(words))):\n",
        "                frequency += 1\n",
        "        freq_col = companyname + \"_freq\"\n",
        "        freq_weight_col = companyname + \"_freq_weight\"\n",
        "        dict[freq_col] = frequency\n",
        "        dict[freq_weight_col] = frequency * weight_dict[category][subcategory]\n",
        "    df1.loc[len(df1)] = dict\n",
        "\n",
        "#df1 = df1.sort_values(\"category\", ascending=True)\n",
        "df1 = df1.sort_values(['category','subcategory'])\n",
        "\n",
        "companys_freq_sum = {name+\"_freq\": \"sum\" for name in collections.keys()}\n",
        "df2 = df1.groupby([\"subcategory\", \"category\"]).agg(companys_freq_sum).reset_index()\n",
        "df2 = df2.sort_values(\"category\", ascending=True)\n",
        "df2_transpose = df2.transpose()\n",
        "df2_transpose = df2_transpose.rename(columns=df2_transpose.iloc[0]).drop(df2_transpose.index[0])\n",
        "\n",
        "company_freq_weight_sum = {name+\"_freq_weight\":\"sum\" for name in collections.keys()}\n",
        "df3 = df1.groupby([\"category\"]).agg(company_freq_weight_sum).reset_index()\n",
        "df3 = df3.sort_values(\"category\", ascending=True)\n",
        "df3_transpose = df3.transpose()\n",
        "df3_transpose = df3_transpose.rename(columns=df3_transpose.iloc[0]).drop(df3_transpose.index[0])\n",
        "\n",
        "df1.to_csv(\"StatisticChart.csv\", index=False)\n",
        "df2_transpose.to_csv(\"Freq.csv\", index=True)\n",
        "df3_transpose.to_csv(\"WeightedFreq.csv\", index=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}